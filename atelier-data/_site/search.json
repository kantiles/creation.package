[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accueil",
    "section": "",
    "text": "Bienvenue sur le site du compte rendu. Utilisez la barre de navigation pour accéder aux différentes parties.\nCe rapport rassemble les notes détaillées sur plusieurs thématiques clés dans le traitement des données, la gestion documentaire, et la production de rapports.\nLes sujets abordés sont :\n\nRecensements et gestion de grandes bases de données\nUtilisation avancée de Quarto et Markdown\nTechniques de scraping en R\nLecture et traitement de PDF en R\nOrganisation documentaire avec Zotero"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Quarto et Markdown",
    "section": "",
    "text": "Quarto repose sur Pandoc : convertisseur universel Markdown vers plusieurs formats (HTML, PDF, Word, etc.).\nÉcrit en Rust, moderne et rapide.\nSupporte plusieurs langages dans les chunks : R, Python, Observable JS, etc."
  },
  {
    "objectID": "quarto.html#contexte",
    "href": "quarto.html#contexte",
    "title": "Scraping Web",
    "section": "",
    "text": "Le scraping est une méthode d’extraction automatique de données…"
  },
  {
    "objectID": "quarto.html#exemple-de-code",
    "href": "quarto.html#exemple-de-code",
    "title": "Scraping Web",
    "section": "Exemple de code",
    "text": "Exemple de code\n\nlibrary(rvest)\nurl &lt;- \"https://example.com\"\nread_html(url)\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;title&gt;Example Domain&lt;/title&gt;\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta http ...\n[2] &lt;body&gt;\\n&lt;div&gt;\\n    &lt;h1&gt;Example Domain&lt;/h1&gt;\\n    &lt;p&gt;This domain is for use ..."
  },
  {
    "objectID": "Kantiles_formation.html",
    "href": "Kantiles_formation.html",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "",
    "text": "Ce rapport rassemble les notes détaillées sur plusieurs thématiques clés dans le traitement des données, la gestion documentaire, et la production de rapports.\nLes sujets abordés sont :\n\nRecensements et gestion de grandes bases de données\nUtilisation avancée de Quarto et Markdown\nTechniques de scraping en R\nLecture et traitement de PDF en R\nOrganisation documentaire avec Zotero"
  },
  {
    "objectID": "Kantiles_formation.html#rapport-annuel",
    "href": "Kantiles_formation.html#rapport-annuel",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "2.1 Rapport annuel",
    "text": "2.1 Rapport annuel\n\n2.1.1 Introduction\nCeci est l’introduction du rapport.\n\n\n2.1.2 Données\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000"
  },
  {
    "objectID": "Kantiles_formation.html#description-des-fichiers",
    "href": "Kantiles_formation.html#description-des-fichiers",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "3.1 Description des fichiers",
    "text": "3.1 Description des fichiers\n\nDeux fichiers principaux : ointerrogationsprincipales et un fichier complémentaire.\nNon diffusés directement au public, servent à produire plusieurs fichiers diffusés.\nDonnées au niveau des cantons, villes, avec parfois localisation précise (unité Iris).\nUtilisation pour mobilités professionnelles et scolaires."
  },
  {
    "objectID": "Kantiles_formation.html#formats-de-diffusion",
    "href": "Kantiles_formation.html#formats-de-diffusion",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "3.2 Formats de diffusion",
    "text": "3.2 Formats de diffusion\n\nFichiers diffusés en CSV et Parquet (format optimisé, indexé, rapide).\nParquet de plus en plus utilisé en statistique.\nDonnées lourdes : environ 26 millions de lignes par fichier.\nUne ligne coûte en ressources ~3 francs (évaluation de coût traitement)."
  },
  {
    "objectID": "Kantiles_formation.html#outils-pour-manipuler",
    "href": "Kantiles_formation.html#outils-pour-manipuler",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "3.3 Outils pour manipuler",
    "text": "3.3 Outils pour manipuler\n\nDugdibby : base de données sur serveur permettant lecture et tri.\nInstallation locale + serveur distant.\nBase orientée colonnes, optimisée pour analyse."
  },
  {
    "objectID": "Kantiles_formation.html#import-et-manipulation-en-r",
    "href": "Kantiles_formation.html#import-et-manipulation-en-r",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "3.4 Import et manipulation en R",
    "text": "3.4 Import et manipulation en R\n\nUtilisation de fonctions glue() pour assembler chaînes de caractères, notamment pour importer.\nUtilisation de here::here() pour gérer chemins absolus et relatifs, éviter erreurs liées aux chemins.\nExemple d’import rapide :\n\nlibrary(here)\ndata_path &lt;- here(\"data\", \"recensement.parquet\")"
  },
  {
    "objectID": "Kantiles_formation.html#vue-générale",
    "href": "Kantiles_formation.html#vue-générale",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "4.1 Vue générale",
    "text": "4.1 Vue générale\n\nQuarto repose sur Pandoc : convertisseur universel Markdown vers plusieurs formats (HTML, PDF, Word, etc.).\nÉcrit en Rust, moderne et rapide.\nSupporte plusieurs langages dans les chunks : R, Python, Observable JS, etc."
  },
  {
    "objectID": "Kantiles_formation.html#workflow",
    "href": "Kantiles_formation.html#workflow",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "4.2 Workflow",
    "text": "4.2 Workflow\n\nQuarto exécute les chunks de code, produit un AST (Abstract Syntax Tree).\nPandoc convertit cet AST en documents finaux (HTML, PDF, etc.)."
  },
  {
    "objectID": "Kantiles_formation.html#comparaison-avec-r-markdown",
    "href": "Kantiles_formation.html#comparaison-avec-r-markdown",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "4.3 Comparaison avec R Markdown",
    "text": "4.3 Comparaison avec R Markdown\n\n\n\n\n\n\n\n\nFonctionnalité\nR Markdown\nQuarto\n\n\n\n\nLangage\nR + Markdown\nRust + Markdown\n\n\nSupport chunks\nR uniquement\nR, Python, JS, autres\n\n\nParamétrage YAML\nBasique\nAvancé\n\n\nSorties\nHTML, PDF, Word\nHTML, PDF, Word, et plus"
  },
  {
    "objectID": "Kantiles_formation.html#création-pdf",
    "href": "Kantiles_formation.html#création-pdf",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "4.4 Création PDF",
    "text": "4.4 Création PDF\n\nPlusieurs méthodes :\n\nLaTeX classique (fiable, mais lourd parfois)\nTypst (nouveau langage, plus rapide)\nPageJS (génération PDF via Chrome)\nImpression design via HTML/Figma"
  },
  {
    "objectID": "Kantiles_formation.html#exemple-de-chunk-r",
    "href": "Kantiles_formation.html#exemple-de-chunk-r",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "4.5 Exemple de chunk R",
    "text": "4.5 Exemple de chunk R\n\n#library(readxl)\n#tables &lt;- read_excel(\"tables_exported.xlsx\")\n#head(tables)"
  },
  {
    "objectID": "Kantiles_formation.html#introduction-2",
    "href": "Kantiles_formation.html#introduction-2",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nLe scraping web consiste à extraire automatiquement des données publiées sur des sites internet. Cette méthode est particulièrement utile lorsque les données ne sont pas disponibles via une API officielle ou sous un format exploitable directement (CSV, JSON, XML, etc.). Elle permet de collecter un large éventail d’informations : fiches produits, listes, tableaux, statistiques, organigrammes, documents, etc.\nCependant, le scraping nécessite une bonne maîtrise des technologies web (HTML, CSS, JavaScript) ainsi qu’une attention aux aspects légaux et éthiques (respect du fichier robots.txt, limitations du site, charge serveur)."
  },
  {
    "objectID": "Kantiles_formation.html#pourquoi-faire-du-scraping",
    "href": "Kantiles_formation.html#pourquoi-faire-du-scraping",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "5.2 Pourquoi faire du scraping ?",
    "text": "5.2 Pourquoi faire du scraping ?\n\nCollecter des données non accessibles autrement, notamment sur des sites qui ne proposent pas d’API ou de fichiers exportables.\n\nAutomatiser la veille documentaire en récupérant régulièrement les nouvelles publications d’un site.\n\nRécupérer des informations structurées dans des pages HTML complexes, par exemple des tableaux ou des fiches détaillées.\n\nEnrichir des jeux de données locaux avec des données publiques sectorielles, comme des organigrammes ou listes d’acteurs."
  },
  {
    "objectID": "Kantiles_formation.html#outils-et-bibliothèques-en-r",
    "href": "Kantiles_formation.html#outils-et-bibliothèques-en-r",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "5.3 Outils et bibliothèques en R",
    "text": "5.3 Outils et bibliothèques en R\n\nrvest : bibliothèque principale pour lire, parser et extraire du contenu HTML. Utilise les sélecteurs CSS ou XPath pour cibler précisément les éléments à extraire.\n\nxml2 : permet de lire et manipuler des documents XML (utile pour lire des sitemaps par exemple).\n\nhttr ou curl : pour gérer les requêtes HTTP, parfois nécessaires pour simuler des sessions, gérer des cookies, ou contourner des protections.\n\npurrr : fonctionnel pour itérer sur des listes d’URLs et appliquer des fonctions de scraping.\n\nExtensions Chrome (Selector Gadget, DevTools) : outils indispensables pour identifier rapidement les sélecteurs CSS et tester les expressions XPath."
  },
  {
    "objectID": "Kantiles_formation.html#bonnes-pratiques-avant-de-commencer",
    "href": "Kantiles_formation.html#bonnes-pratiques-avant-de-commencer",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "5.4 Bonnes pratiques avant de commencer",
    "text": "5.4 Bonnes pratiques avant de commencer\n\nVérifier le fichier robots.txt du site (ex. https://www.site.com/robots.txt) qui liste les parties autorisées ou interdites au scraping. Respecter ces consignes est crucial pour éviter tout litige légal.\n\nAnalyser la structure du site : repérer les pages d’index, les pages détaillées, les liens à suivre, et identifier les sélecteurs CSS/XPath précis.\n\nConsulter le sitemap XML si disponible, car il facilite la récupération automatique des URLs à scraper.\n\nLimiter la fréquence des requêtes pour ne pas surcharger le serveur (exemple : pause entre requêtes avec Sys.sleep()).\n\nTester et valider chaque étape manuellement avant d’automatiser à grande échelle.\n\nDocumenter le script pour faciliter la maintenance et la mise à jour si la structure du site change."
  },
  {
    "objectID": "Kantiles_formation.html#exemple-complet-récupération-et-scraping-de-fiches-sur-cheese.com",
    "href": "Kantiles_formation.html#exemple-complet-récupération-et-scraping-de-fiches-sur-cheese.com",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "5.5 Exemple complet : récupération et scraping de fiches sur cheese.com",
    "text": "5.5 Exemple complet : récupération et scraping de fiches sur cheese.com\n\n5.5.1 Étape 1 : récupération des URLs via sitemap\n\nlibrary(xml2)\nlibrary(magrittr)\n\nurl_sitemap &lt;- \"https://www.cheese.com/sitemap.xml\"\nsitemap &lt;- read_xml(url_sitemap)\n\n# Extraction des URLs du sitemap\nurls &lt;- xml_find_all(sitemap, \"//url/loc\") %&gt;% xml_text()\nhead(urls, 5)\n\ncharacter(0)\n\nlibrary(rvest)\nlibrary(dplyr)\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\nurl_annuaire &lt;- \"https://www.cheese.com/all-cheeses/\"\npage &lt;- read_html(url_annuaire)\n\n# Extraction des liens vers chaque fiche\nliens_fromages &lt;- page %&gt;%\n  html_nodes(\".cheeseList a\") %&gt;%\n  html_attr(\"href\")\n\n# Nettoyage des liens pour avoir URL absolue\nliens_fromages &lt;- paste0(\"https://www.cheese.com\", liens_fromages)\nhead(liens_fromages, 5)\n\n[1] \"https://www.cheese.com\"\n\nlibrary(rvest)\nlibrary(dplyr)\n\nurl_annuaire &lt;- \"https://www.cheese.com/all-cheeses/\"\npage &lt;- read_html(url_annuaire)\n\n# Extraction des liens vers chaque fiche\nliens_fromages &lt;- page %&gt;%\n  html_nodes(\".cheeseList a\") %&gt;%\n  html_attr(\"href\")\n\n# Nettoyage des liens pour avoir URL absolue\nliens_fromages &lt;- paste0(\"https://www.cheese.com\", liens_fromages)\nhead(liens_fromages, 5)\n\n[1] \"https://www.cheese.com\""
  },
  {
    "objectID": "Kantiles_formation.html#outils",
    "href": "Kantiles_formation.html#outils",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "6.1 Outils",
    "text": "6.1 Outils\n\ntesseract : outil OCR pour extraire du texte à partir d’images contenues dans des PDF.\ntabulizer (package R) : extraction de tableaux depuis des PDF, particulièrement efficace pour les tableaux “screenshot” (captures d’écran intégrées dans PDF).\npdftools : manipulation basique et extraction de texte dans les PDF."
  },
  {
    "objectID": "Kantiles_formation.html#usages",
    "href": "Kantiles_formation.html#usages",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "6.2 Usages",
    "text": "6.2 Usages\n\nExtraction de tableaux complexes, notamment lorsque les données sont intégrées en image (screenshot).\nLe package tabulizer est utile pour extraire ces tableaux, contrairement à l’OCR qui fonctionne moins bien sur ce type de données.\nVérifier la cohérence des tailles de listes extraites (ex. colonnes de tableaux) pour éviter les erreurs de décalage ou pertes de données.\nGérer les valeurs manquantes, car en HTML/texte brut, il n’y a pas de NA natif.\nUtile pour enrichir des données sectorielles ou locales à partir de rapports PDF."
  },
  {
    "objectID": "Kantiles_formation.html#cas-pratique",
    "href": "Kantiles_formation.html#cas-pratique",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "6.3 Cas pratique",
    "text": "6.3 Cas pratique\n\nTester sur des sites institutionnels comme l’INSEE en live.\nNettoyer et structurer les données extraites pour créer des jeux de données propres.\nÊtre vigilant aux problèmes liés à la mise en forme des PDF (listes, colonnes, espacements).\n\n\n# library(pdftools)\n\n# Exemple d'extraction texte d'un PDF local\n# texte_pdf &lt;- pdf_text(\"exemple_rapport.pdf\")\n\n# Affichage des 500 premiers caractères de la première page\n# cat(substr(texte_pdf[1], 1, 500))\n\n\n# library(tabulizer)\n\n# Extraction des tableaux d'un PDF\n# tables &lt;- extract_tables(\"exemple_rapport.pdf\")\n\n# Affichage du premier tableau extrait\n# print(tables[[1]])\n\n\n# library(tesseract)\n\n# Extraction de texte depuis une image (capture ou PDF image)\n# texte_ocr &lt;- ocr(\"exemple_image_page.png\")\n\n# cat(texte_ocr)"
  },
  {
    "objectID": "Kantiles_formation.html#module-zotero-veille-documentaire",
    "href": "Kantiles_formation.html#module-zotero-veille-documentaire",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.1 Module : Zotero & Veille documentaire",
    "text": "7.1 Module : Zotero & Veille documentaire\n\nJuin 2025\nEmmanuel Herbepin et Victoire Chatain"
  },
  {
    "objectID": "Kantiles_formation.html#présentation-générale-de-zotero",
    "href": "Kantiles_formation.html#présentation-générale-de-zotero",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.2 Présentation générale de Zotero",
    "text": "7.2 Présentation générale de Zotero\n\nZotero est un outil de gestion bibliographique gratuit, open source, et très adapté pour :\n\nLa veille documentaire structurée.\n\nLa gestion collaborative de références (études, articles, rapports, etc.).\n\nLa constitution d’une base de connaissance dynamique."
  },
  {
    "objectID": "Kantiles_formation.html#fonctionnement-de-base",
    "href": "Kantiles_formation.html#fonctionnement-de-base",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.3 Fonctionnement de base",
    "text": "7.3 Fonctionnement de base\n\nOrganisation par dossiers et sous-dossiers.\n\nAjout de documents :\n\nDepuis un plugin navigateur (fonctionne très bien avec Cairn, par exemple).\n\nManuellement (ajout de PDF, capture de page, etc.).\n\n\nChaque document peut contenir :\n\nDes métadonnées (titre, auteur, date…).\n\nDes pièces jointes (PDF, notes, etc.).\n\nDes commentaires et surlignages collaboratifs."
  },
  {
    "objectID": "Kantiles_formation.html#travail-collaboratif",
    "href": "Kantiles_formation.html#travail-collaboratif",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.4 Travail collaboratif",
    "text": "7.4 Travail collaboratif\n\nBibliothèque de groupe (via Zotero Web) :\n\nParamétrage des droits par membre.\n\nSynchronisation automatique.\n\nPartage des annotations visibles par tous (identification de l’auteur, surlignages, commentaires).\n\n\nUtilisable comme un Drive académique orienté contenu structuré."
  },
  {
    "objectID": "Kantiles_formation.html#fonctionnalités-utiles",
    "href": "Kantiles_formation.html#fonctionnalités-utiles",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.5 Fonctionnalités utiles",
    "text": "7.5 Fonctionnalités utiles\n\nTags / marqueurs pour organiser les contenus.\n\nGénération automatique de bibliographies (choix du format).\n\nFormat de citation personnalisable (utile pour livrables, publications, rapports).\n\nFormat BibTeX disponible → interopérable avec R, Python, LaTeX."
  },
  {
    "objectID": "Kantiles_formation.html#cas-dusage-professionnel-veille-sur-lemploi-la-formation",
    "href": "Kantiles_formation.html#cas-dusage-professionnel-veille-sur-lemploi-la-formation",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.6 Cas d’usage professionnel : veille sur l’emploi & la formation",
    "text": "7.6 Cas d’usage professionnel : veille sur l’emploi & la formation\n\nObjectif :\nConstruire une base documentaire cohérente, exhaustive, structurée, réutilisable et dynamique sur les thématiques métier.\n\nExemples :\n\nÉtudes sur les compétences dans les métiers de la santé.\n\nPublications régionales et nationales : DREES, DARES, INSEE, France Travail, etc.\n\nMémoires, études locales, travaux des observatoires OPCO, InterCarifOref, France Compétences…\n\n\nOrganisation :\n\nHiérarchisation des sources.\n\nSélection raisonnée des institutions clés.\n\nVeille structurée sur 15 000 à 20 000 études (sur 20 ans) de 4 à 8 pages chacune."
  },
  {
    "objectID": "Kantiles_formation.html#méthodologie-recommandée",
    "href": "Kantiles_formation.html#méthodologie-recommandée",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.7 Méthodologie recommandée",
    "text": "7.7 Méthodologie recommandée\n\nÉtape 1 : Constitution du corpus\n\nListe prioritaire : DREES, DARES, INSEE, France Travail, Trésor, OPCO.\n\nActeurs régionaux : InterCarifOref, collectivités territoriales.\n\nIndexation manuelle + flux RSS + scraping raisonné.\n\n\nÉtape 2 : Intégration dans Zotero\n\nPar scraping ou via export BibTeX → import dans Zotero.\n\nUtilisation des API Zotero et/ou packages dédiés pour automatiser.\n\n\nÉtape 3 : Structuration documentaire\n\nTags thématiques.\n\nCatégorisation par année, région, institution.\n\nExport possible vers livrable interne ou outil de visualisation (type Quarto ou R Markdown)."
  },
  {
    "objectID": "Kantiles_formation.html#vers-une-base-documentaire-dynamique",
    "href": "Kantiles_formation.html#vers-une-base-documentaire-dynamique",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.8 Vers une base documentaire dynamique",
    "text": "7.8 Vers une base documentaire dynamique\n\nObjectif : passer du statique au dynamique.\n\nIntégrer les études dans une base réinterrogeable (dashboard, moteur de recherche, analyse de tendance…).\n\nBase centrale pour fonction observatoire interne.\n\nExemples d’usages :\n\nListe des études par année.\n\nFocus régional ou sectoriel.\n\nSuivi des politiques publiques ou signaux faibles."
  },
  {
    "objectID": "Kantiles_formation.html#à-creuser-aller-plus-loin",
    "href": "Kantiles_formation.html#à-creuser-aller-plus-loin",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "7.9 À creuser / Aller plus loin",
    "text": "7.9 À creuser / Aller plus loin\n\nAPI Zotero pour automatiser des chargements et extractions.\n\nConnecteurs avec Zotero depuis R (rbbt, zoteroR) ou Python.\n\nScraping bibliographique ciblé sur les bons sites.\n\nIntégration avec un outil de visualisation dynamique (Quarto, Observable, Shiny…)."
  },
  {
    "objectID": "Kantiles_formation.html#lecture-et-extraction-de-sitemap-xml",
    "href": "Kantiles_formation.html#lecture-et-extraction-de-sitemap-xml",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "8.1 Lecture et extraction de sitemap XML",
    "text": "8.1 Lecture et extraction de sitemap XML\n\nlibrary(rvest)\nlibrary(xml2)\n\nurl_sitemap &lt;- \"https://www.cheese.com/sitemap.xml\"\nsitemap &lt;- read_xml(url_sitemap)\nurls &lt;- xml_find_all(sitemap, \"//url/loc\") %&gt;% xml_text()\nhead(urls)\n\ncharacter(0)"
  },
  {
    "objectID": "Kantiles_formation.html#quarto",
    "href": "Kantiles_formation.html#quarto",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "8.2 Quarto",
    "text": "8.2 Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Kantiles_formation.html#running-code",
    "href": "Kantiles_formation.html#running-code",
    "title": "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero",
    "section": "8.3 Running Code",
    "text": "8.3 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:"
  },
  {
    "objectID": "zotero.html",
    "href": "zotero.html",
    "title": "zotero",
    "section": "",
    "text": "Juin 2025\nEmmanuel Herbepin et Victoire Chatain\n\n\n\n\n\n\nZotero est un outil de gestion bibliographique gratuit, open source, et très adapté pour :\n\nLa veille documentaire structurée.\n\nLa gestion collaborative de références (études, articles, rapports, etc.).\n\nLa constitution d’une base de connaissance dynamique.\n\n\n\n\n\n\n\nOrganisation par dossiers et sous-dossiers.\n\nAjout de documents :\n\nDepuis un plugin navigateur (fonctionne très bien avec Cairn, par exemple).\n\nManuellement (ajout de PDF, capture de page, etc.).\n\n\nChaque document peut contenir :\n\nDes métadonnées (titre, auteur, date…).\n\nDes pièces jointes (PDF, notes, etc.).\n\nDes commentaires et surlignages collaboratifs.\n\n\n\n\n\n\n\nBibliothèque de groupe (via Zotero Web) :\n\nParamétrage des droits par membre.\n\nSynchronisation automatique.\n\nPartage des annotations visibles par tous (identification de l’auteur, surlignages, commentaires).\n\n\nUtilisable comme un Drive académique orienté contenu structuré.\n\n\n\n\n\n\nTags / marqueurs pour organiser les contenus.\n\nGénération automatique de bibliographies (choix du format).\n\nFormat de citation personnalisable (utile pour livrables, publications, rapports).\n\nFormat BibTeX disponible → interopérable avec R, Python, LaTeX.\n\n\n\n\n\n\nObjectif :\nConstruire une base documentaire cohérente, exhaustive, structurée, réutilisable et dynamique sur les thématiques métier.\n\nExemples :\n\nÉtudes sur les compétences dans les métiers de la santé.\n\nPublications régionales et nationales : DREES, DARES, INSEE, France Travail, etc.\n\nMémoires, études locales, travaux des observatoires OPCO, InterCarifOref, France Compétences…\n\n\nOrganisation :\n\nHiérarchisation des sources.\n\nSélection raisonnée des institutions clés.\n\nVeille structurée sur 15 000 à 20 000 études (sur 20 ans) de 4 à 8 pages chacune.\n\n\n\n\n\n\n\nÉtape 1 : Constitution du corpus\n\nListe prioritaire : DREES, DARES, INSEE, France Travail, Trésor, OPCO.\n\nActeurs régionaux : InterCarifOref, collectivités territoriales.\n\nIndexation manuelle + flux RSS + scraping raisonné.\n\n\nÉtape 2 : Intégration dans Zotero\n\nPar scraping ou via export BibTeX → import dans Zotero.\n\nUtilisation des API Zotero et/ou packages dédiés pour automatiser.\n\n\nÉtape 3 : Structuration documentaire\n\nTags thématiques.\n\nCatégorisation par année, région, institution.\n\nExport possible vers livrable interne ou outil de visualisation (type Quarto ou R Markdown).\n\n\n\n\n\n\n\nObjectif : passer du statique au dynamique.\n\nIntégrer les études dans une base réinterrogeable (dashboard, moteur de recherche, analyse de tendance…).\n\nBase centrale pour fonction observatoire interne.\n\nExemples d’usages :\n\nListe des études par année.\n\nFocus régional ou sectoriel.\n\nSuivi des politiques publiques ou signaux faibles.\n\n\n\n\n\n\n\nAPI Zotero pour automatiser des chargements et extractions.\n\nConnecteurs avec Zotero depuis R (rbbt, zoteroR) ou Python.\n\nScraping bibliographique ciblé sur les bons sites.\n\nIntégration avec un outil de visualisation dynamique (Quarto, Observable, Shiny…)."
  },
  {
    "objectID": "zotero.html#quarto",
    "href": "zotero.html#quarto",
    "title": "zotero",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "zotero.html#running-code",
    "href": "zotero.html#running-code",
    "title": "zotero",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "zotero.html#module-zotero-veille-documentaire",
    "href": "zotero.html#module-zotero-veille-documentaire",
    "title": "zotero",
    "section": "",
    "text": "Juin 2025\nEmmanuel Herbepin et Victoire Chatain"
  },
  {
    "objectID": "zotero.html#présentation-générale-de-zotero",
    "href": "zotero.html#présentation-générale-de-zotero",
    "title": "zotero",
    "section": "",
    "text": "Zotero est un outil de gestion bibliographique gratuit, open source, et très adapté pour :\n\nLa veille documentaire structurée.\n\nLa gestion collaborative de références (études, articles, rapports, etc.).\n\nLa constitution d’une base de connaissance dynamique."
  },
  {
    "objectID": "zotero.html#fonctionnement-de-base",
    "href": "zotero.html#fonctionnement-de-base",
    "title": "zotero",
    "section": "",
    "text": "Organisation par dossiers et sous-dossiers.\n\nAjout de documents :\n\nDepuis un plugin navigateur (fonctionne très bien avec Cairn, par exemple).\n\nManuellement (ajout de PDF, capture de page, etc.).\n\n\nChaque document peut contenir :\n\nDes métadonnées (titre, auteur, date…).\n\nDes pièces jointes (PDF, notes, etc.).\n\nDes commentaires et surlignages collaboratifs."
  },
  {
    "objectID": "zotero.html#travail-collaboratif",
    "href": "zotero.html#travail-collaboratif",
    "title": "zotero",
    "section": "",
    "text": "Bibliothèque de groupe (via Zotero Web) :\n\nParamétrage des droits par membre.\n\nSynchronisation automatique.\n\nPartage des annotations visibles par tous (identification de l’auteur, surlignages, commentaires).\n\n\nUtilisable comme un Drive académique orienté contenu structuré."
  },
  {
    "objectID": "zotero.html#fonctionnalités-utiles",
    "href": "zotero.html#fonctionnalités-utiles",
    "title": "zotero",
    "section": "",
    "text": "Tags / marqueurs pour organiser les contenus.\n\nGénération automatique de bibliographies (choix du format).\n\nFormat de citation personnalisable (utile pour livrables, publications, rapports).\n\nFormat BibTeX disponible → interopérable avec R, Python, LaTeX."
  },
  {
    "objectID": "zotero.html#cas-dusage-professionnel-veille-sur-lemploi-la-formation",
    "href": "zotero.html#cas-dusage-professionnel-veille-sur-lemploi-la-formation",
    "title": "zotero",
    "section": "",
    "text": "Objectif :\nConstruire une base documentaire cohérente, exhaustive, structurée, réutilisable et dynamique sur les thématiques métier.\n\nExemples :\n\nÉtudes sur les compétences dans les métiers de la santé.\n\nPublications régionales et nationales : DREES, DARES, INSEE, France Travail, etc.\n\nMémoires, études locales, travaux des observatoires OPCO, InterCarifOref, France Compétences…\n\n\nOrganisation :\n\nHiérarchisation des sources.\n\nSélection raisonnée des institutions clés.\n\nVeille structurée sur 15 000 à 20 000 études (sur 20 ans) de 4 à 8 pages chacune."
  },
  {
    "objectID": "zotero.html#méthodologie-recommandée",
    "href": "zotero.html#méthodologie-recommandée",
    "title": "zotero",
    "section": "",
    "text": "Étape 1 : Constitution du corpus\n\nListe prioritaire : DREES, DARES, INSEE, France Travail, Trésor, OPCO.\n\nActeurs régionaux : InterCarifOref, collectivités territoriales.\n\nIndexation manuelle + flux RSS + scraping raisonné.\n\n\nÉtape 2 : Intégration dans Zotero\n\nPar scraping ou via export BibTeX → import dans Zotero.\n\nUtilisation des API Zotero et/ou packages dédiés pour automatiser.\n\n\nÉtape 3 : Structuration documentaire\n\nTags thématiques.\n\nCatégorisation par année, région, institution.\n\nExport possible vers livrable interne ou outil de visualisation (type Quarto ou R Markdown)."
  },
  {
    "objectID": "zotero.html#vers-une-base-documentaire-dynamique",
    "href": "zotero.html#vers-une-base-documentaire-dynamique",
    "title": "zotero",
    "section": "",
    "text": "Objectif : passer du statique au dynamique.\n\nIntégrer les études dans une base réinterrogeable (dashboard, moteur de recherche, analyse de tendance…).\n\nBase centrale pour fonction observatoire interne.\n\nExemples d’usages :\n\nListe des études par année.\n\nFocus régional ou sectoriel.\n\nSuivi des politiques publiques ou signaux faibles."
  },
  {
    "objectID": "zotero.html#à-creuser-aller-plus-loin",
    "href": "zotero.html#à-creuser-aller-plus-loin",
    "title": "zotero",
    "section": "",
    "text": "API Zotero pour automatiser des chargements et extractions.\n\nConnecteurs avec Zotero depuis R (rbbt, zoteroR) ou Python.\n\nScraping bibliographique ciblé sur les bons sites.\n\nIntégration avec un outil de visualisation dynamique (Quarto, Observable, Shiny…)."
  },
  {
    "objectID": "scrap.html",
    "href": "scrap.html",
    "title": "Scrap",
    "section": "",
    "text": "Le scraping web consiste à extraire automatiquement des données publiées sur des sites internet. Cette méthode est particulièrement utile lorsque les données ne sont pas disponibles via une API officielle ou sous un format exploitable directement (CSV, JSON, XML, etc.). Elle permet de collecter un large éventail d’informations : fiches produits, listes, tableaux, statistiques, organigrammes, documents, etc.\nCependant, le scraping nécessite une bonne maîtrise des technologies web (HTML, CSS, JavaScript) ainsi qu’une attention aux aspects légaux et éthiques (respect du fichier robots.txt, limitations du site, charge serveur).\n\n\n\n\n\nCollecter des données non accessibles autrement, notamment sur des sites qui ne proposent pas d’API ou de fichiers exportables.\n\nAutomatiser la veille documentaire en récupérant régulièrement les nouvelles publications d’un site.\n\nRécupérer des informations structurées dans des pages HTML complexes, par exemple des tableaux ou des fiches détaillées.\n\nEnrichir des jeux de données locaux avec des données publiques sectorielles, comme des organigrammes ou listes d’acteurs.\n\n\n\n\n\n\nrvest : bibliothèque principale pour lire, parser et extraire du contenu HTML. Utilise les sélecteurs CSS ou XPath pour cibler précisément les éléments à extraire.\n\nxml2 : permet de lire et manipuler des documents XML (utile pour lire des sitemaps par exemple).\n\nhttr ou curl : pour gérer les requêtes HTTP, parfois nécessaires pour simuler des sessions, gérer des cookies, ou contourner des protections.\n\npurrr : fonctionnel pour itérer sur des listes d’URLs et appliquer des fonctions de scraping.\n\nExtensions Chrome (Selector Gadget, DevTools) : outils indispensables pour identifier rapidement les sélecteurs CSS et tester les expressions XPath.\n\n\n\n\n\n\nVérifier le fichier robots.txt du site (ex. https://www.site.com/robots.txt) qui liste les parties autorisées ou interdites au scraping. Respecter ces consignes est crucial pour éviter tout litige légal.\n\nAnalyser la structure du site : repérer les pages d’index, les pages détaillées, les liens à suivre, et identifier les sélecteurs CSS/XPath précis.\n\nConsulter le sitemap XML si disponible, car il facilite la récupération automatique des URLs à scraper.\n\nLimiter la fréquence des requêtes pour ne pas surcharger le serveur (exemple : pause entre requêtes avec Sys.sleep()).\n\nTester et valider chaque étape manuellement avant d’automatiser à grande échelle.\n\nDocumenter le script pour faciliter la maintenance et la mise à jour si la structure du site change.\n\n\n\n\n\n\n\n\nlibrary(xml2)\nlibrary(magrittr)\n\nurl_sitemap &lt;- \"https://www.cheese.com/sitemap.xml\"\nsitemap &lt;- read_xml(url_sitemap)\n\n# Extraction des URLs du sitemap\nurls &lt;- xml_find_all(sitemap, \"//url/loc\") %&gt;% xml_text()\nhead(urls, 5)\n\ncharacter(0)\n\nlibrary(rvest)\nlibrary(dplyr)\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\nurl_annuaire &lt;- \"https://www.cheese.com/all-cheeses/\"\npage &lt;- read_html(url_annuaire)\n\n# Extraction des liens vers chaque fiche\nliens_fromages &lt;- page %&gt;%\n  html_nodes(\".cheeseList a\") %&gt;%\n  html_attr(\"href\")\n\n# Nettoyage des liens pour avoir URL absolue\nliens_fromages &lt;- paste0(\"https://www.cheese.com\", liens_fromages)\nhead(liens_fromages, 5)\n\n[1] \"https://www.cheese.com\"\n\nlibrary(rvest)\nlibrary(dplyr)\n\nurl_annuaire &lt;- \"https://www.cheese.com/all-cheeses/\"\npage &lt;- read_html(url_annuaire)\n\n# Extraction des liens vers chaque fiche\nliens_fromages &lt;- page %&gt;%\n  html_nodes(\".cheeseList a\") %&gt;%\n  html_attr(\"href\")\n\n# Nettoyage des liens pour avoir URL absolue\nliens_fromages &lt;- paste0(\"https://www.cheese.com\", liens_fromages)\nhead(liens_fromages, 5)\n\n[1] \"https://www.cheese.com\""
  },
  {
    "objectID": "scrap.html#introduction",
    "href": "scrap.html#introduction",
    "title": "Scrap",
    "section": "",
    "text": "Le scraping web consiste à extraire automatiquement des données publiées sur des sites internet. Cette méthode est particulièrement utile lorsque les données ne sont pas disponibles via une API officielle ou sous un format exploitable directement (CSV, JSON, XML, etc.). Elle permet de collecter un large éventail d’informations : fiches produits, listes, tableaux, statistiques, organigrammes, documents, etc.\nCependant, le scraping nécessite une bonne maîtrise des technologies web (HTML, CSS, JavaScript) ainsi qu’une attention aux aspects légaux et éthiques (respect du fichier robots.txt, limitations du site, charge serveur)."
  },
  {
    "objectID": "scrap.html#pourquoi-faire-du-scraping",
    "href": "scrap.html#pourquoi-faire-du-scraping",
    "title": "Scrap",
    "section": "",
    "text": "Collecter des données non accessibles autrement, notamment sur des sites qui ne proposent pas d’API ou de fichiers exportables.\n\nAutomatiser la veille documentaire en récupérant régulièrement les nouvelles publications d’un site.\n\nRécupérer des informations structurées dans des pages HTML complexes, par exemple des tableaux ou des fiches détaillées.\n\nEnrichir des jeux de données locaux avec des données publiques sectorielles, comme des organigrammes ou listes d’acteurs."
  },
  {
    "objectID": "scrap.html#outils-et-bibliothèques-en-r",
    "href": "scrap.html#outils-et-bibliothèques-en-r",
    "title": "Scrap",
    "section": "",
    "text": "rvest : bibliothèque principale pour lire, parser et extraire du contenu HTML. Utilise les sélecteurs CSS ou XPath pour cibler précisément les éléments à extraire.\n\nxml2 : permet de lire et manipuler des documents XML (utile pour lire des sitemaps par exemple).\n\nhttr ou curl : pour gérer les requêtes HTTP, parfois nécessaires pour simuler des sessions, gérer des cookies, ou contourner des protections.\n\npurrr : fonctionnel pour itérer sur des listes d’URLs et appliquer des fonctions de scraping.\n\nExtensions Chrome (Selector Gadget, DevTools) : outils indispensables pour identifier rapidement les sélecteurs CSS et tester les expressions XPath."
  },
  {
    "objectID": "scrap.html#bonnes-pratiques-avant-de-commencer",
    "href": "scrap.html#bonnes-pratiques-avant-de-commencer",
    "title": "Scrap",
    "section": "",
    "text": "Vérifier le fichier robots.txt du site (ex. https://www.site.com/robots.txt) qui liste les parties autorisées ou interdites au scraping. Respecter ces consignes est crucial pour éviter tout litige légal.\n\nAnalyser la structure du site : repérer les pages d’index, les pages détaillées, les liens à suivre, et identifier les sélecteurs CSS/XPath précis.\n\nConsulter le sitemap XML si disponible, car il facilite la récupération automatique des URLs à scraper.\n\nLimiter la fréquence des requêtes pour ne pas surcharger le serveur (exemple : pause entre requêtes avec Sys.sleep()).\n\nTester et valider chaque étape manuellement avant d’automatiser à grande échelle.\n\nDocumenter le script pour faciliter la maintenance et la mise à jour si la structure du site change."
  },
  {
    "objectID": "scrap.html#exemple-complet-récupération-et-scraping-de-fiches-sur-cheese.com",
    "href": "scrap.html#exemple-complet-récupération-et-scraping-de-fiches-sur-cheese.com",
    "title": "Scrap",
    "section": "",
    "text": "library(xml2)\nlibrary(magrittr)\n\nurl_sitemap &lt;- \"https://www.cheese.com/sitemap.xml\"\nsitemap &lt;- read_xml(url_sitemap)\n\n# Extraction des URLs du sitemap\nurls &lt;- xml_find_all(sitemap, \"//url/loc\") %&gt;% xml_text()\nhead(urls, 5)\n\ncharacter(0)\n\nlibrary(rvest)\nlibrary(dplyr)\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\nurl_annuaire &lt;- \"https://www.cheese.com/all-cheeses/\"\npage &lt;- read_html(url_annuaire)\n\n# Extraction des liens vers chaque fiche\nliens_fromages &lt;- page %&gt;%\n  html_nodes(\".cheeseList a\") %&gt;%\n  html_attr(\"href\")\n\n# Nettoyage des liens pour avoir URL absolue\nliens_fromages &lt;- paste0(\"https://www.cheese.com\", liens_fromages)\nhead(liens_fromages, 5)\n\n[1] \"https://www.cheese.com\"\n\nlibrary(rvest)\nlibrary(dplyr)\n\nurl_annuaire &lt;- \"https://www.cheese.com/all-cheeses/\"\npage &lt;- read_html(url_annuaire)\n\n# Extraction des liens vers chaque fiche\nliens_fromages &lt;- page %&gt;%\n  html_nodes(\".cheeseList a\") %&gt;%\n  html_attr(\"href\")\n\n# Nettoyage des liens pour avoir URL absolue\nliens_fromages &lt;- paste0(\"https://www.cheese.com\", liens_fromages)\nhead(liens_fromages, 5)\n\n[1] \"https://www.cheese.com\""
  },
  {
    "objectID": "scrap.html#outils",
    "href": "scrap.html#outils",
    "title": "Scrap",
    "section": "Outils",
    "text": "Outils\n\ntesseract : outil OCR pour extraire du texte à partir d’images contenues dans des PDF.\ntabulizer (package R) : extraction de tableaux depuis des PDF, particulièrement efficace pour les tableaux “screenshot” (captures d’écran intégrées dans PDF).\npdftools : manipulation basique et extraction de texte dans les PDF."
  },
  {
    "objectID": "scrap.html#usages",
    "href": "scrap.html#usages",
    "title": "Scrap",
    "section": "Usages",
    "text": "Usages\n\nExtraction de tableaux complexes, notamment lorsque les données sont intégrées en image (screenshot).\nLe package tabulizer est utile pour extraire ces tableaux, contrairement à l’OCR qui fonctionne moins bien sur ce type de données.\nVérifier la cohérence des tailles de listes extraites (ex. colonnes de tableaux) pour éviter les erreurs de décalage ou pertes de données.\nGérer les valeurs manquantes, car en HTML/texte brut, il n’y a pas de NA natif.\nUtile pour enrichir des données sectorielles ou locales à partir de rapports PDF."
  },
  {
    "objectID": "scrap.html#cas-pratique",
    "href": "scrap.html#cas-pratique",
    "title": "Scrap",
    "section": "Cas pratique",
    "text": "Cas pratique\n\nTester sur des sites institutionnels comme l’INSEE en live.\nNettoyer et structurer les données extraites pour créer des jeux de données propres.\nÊtre vigilant aux problèmes liés à la mise en forme des PDF (listes, colonnes, espacements).\n\n\n# library(pdftools)\n\n# Exemple d'extraction texte d'un PDF local\n# texte_pdf &lt;- pdf_text(\"exemple_rapport.pdf\")\n\n# Affichage des 500 premiers caractères de la première page\n# cat(substr(texte_pdf[1], 1, 500))\n\n\n# library(tabulizer)\n\n# Extraction des tableaux d'un PDF\n# tables &lt;- extract_tables(\"exemple_rapport.pdf\")\n\n# Affichage du premier tableau extrait\n# print(tables[[1]])\n\n\n# library(tesseract)\n\n# Extraction de texte depuis une image (capture ou PDF image)\n# texte_ocr &lt;- ocr(\"exemple_image_page.png\")\n\n# cat(texte_ocr)"
  },
  {
    "objectID": "parquet.html",
    "href": "parquet.html",
    "title": "parquet",
    "section": "",
    "text": "Deux fichiers principaux : ointerrogationsprincipales et un fichier complémentaire.\nNon diffusés directement au public, servent à produire plusieurs fichiers diffusés.\nDonnées au niveau des cantons, villes, avec parfois localisation précise (unité Iris).\nUtilisation pour mobilités professionnelles et scolaires."
  },
  {
    "objectID": "parquet.html#description-des-fichiers",
    "href": "parquet.html#description-des-fichiers",
    "title": "parquet",
    "section": "",
    "text": "Deux fichiers principaux : ointerrogationsprincipales et un fichier complémentaire.\nNon diffusés directement au public, servent à produire plusieurs fichiers diffusés.\nDonnées au niveau des cantons, villes, avec parfois localisation précise (unité Iris).\nUtilisation pour mobilités professionnelles et scolaires."
  },
  {
    "objectID": "parquet.html#formats-de-diffusion",
    "href": "parquet.html#formats-de-diffusion",
    "title": "parquet",
    "section": "Formats de diffusion",
    "text": "Formats de diffusion\n\nFichiers diffusés en CSV et Parquet (format optimisé, indexé, rapide).\nParquet de plus en plus utilisé en statistique.\nDonnées lourdes : environ 26 millions de lignes par fichier.\nUne ligne coûte en ressources ~3 francs (évaluation de coût traitement)."
  },
  {
    "objectID": "parquet.html#outils-pour-manipuler",
    "href": "parquet.html#outils-pour-manipuler",
    "title": "parquet",
    "section": "Outils pour manipuler",
    "text": "Outils pour manipuler\n\nDugdibby : base de données sur serveur permettant lecture et tri.\nInstallation locale + serveur distant.\nBase orientée colonnes, optimisée pour analyse."
  },
  {
    "objectID": "parquet.html#import-et-manipulation-en-r",
    "href": "parquet.html#import-et-manipulation-en-r",
    "title": "parquet",
    "section": "Import et manipulation en R",
    "text": "Import et manipulation en R\n\nUtilisation de fonctions glue() pour assembler chaînes de caractères, notamment pour importer.\nUtilisation de here::here() pour gérer chemins absolus et relatifs, éviter erreurs liées aux chemins.\nExemple d’import rapide :\n\nlibrary(here)\ndata_path &lt;- here(\"data\", \"recensement.parquet\")"
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "API",
    "section": "",
    "text": "Lorsqu’on veut récuopérer des données sur internet, la première question à se poser est :\nExiste-t-il un service pour ?\nDans le cas le plus simple une base est disponible en accès libre (data.gouv, insee.fr, etc.). Si les informations ne sont pas disponibles, peut être existe-t-il un service de requête des données : une API (Application Programming Interface).\n\n\nUne API est une interface « clairement délimitée par laquelle un logiciel offre des services à d’autres logiciels ».\n\nChaque API est accompagnée d’une documentation. Il est souvent nécessaire de s’y référer pour former des requêtes valables.\nUne API attend des paramètres précis qui peuvent varier selon les données que l’on demande.\nNotre exemple : France Travail propose un certain nombre d’API pour des services différents et des clients différents (particuliers, employeurs, etc.). Nous nous intéresserons à l’API qui donne accès à des informations territoriales recensées par France Travail\n\nSi R n’est pas le meilleur langage pour requêter des API, il existe quelques packages qui facilitent le travail. Nous utiliserons httr2."
  },
  {
    "objectID": "api.html#blablabla",
    "href": "api.html#blablabla",
    "title": "api",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "api.html#running-code",
    "href": "api.html#running-code",
    "title": "api",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "article.html",
    "href": "article.html",
    "title": "Kantiles fête ses 10 ans",
    "section": "",
    "text": "Par notre correspondant à Gouvieux, édition du 5 juin 2034\n\n\n“Ce qui compte, ce n’est pas de produire plus de données, mais de les faire parler autrement — avec précision, transparence, et dans l’intérêt public. C’est là que se joue notre valeur ajoutée : dans la rigueur des analyses, la diversité des sources, et la lisibilité des résultats.”\n— Thomas Vroylandt, cofondateur de Kantiles\n\n\n\nUne décennie au service des politiques sociales et de l’emploi\nEn 2024, dans leur salon de Gouvieux (Oise), Justine et Thomas Vroylandt fondent Kantiles. Leur intuition est simple : les politiques publiques regorgent de données, mais manquent d’outils pour les transformer en savoirs utiles à l’action.\nLui, statisticien et développeur, pilote les analyses, les modèles, les méthodes. Elle, issue du secteur social, connaît les rouages des institutions, les usages de terrain, les besoins du service public. Ensemble, ils créent une structure atypique : experte, engagée, et ancrée dans la réalité des politiques sociales.\n\nDeux profils complémentaires, une ambition commune : réconcilier expertise statistique, éthique publique et action sociale.\n\n\n\n\nDe Gouvieux à l’international : une trajectoire ascendante\nEn 2025, les premiers stagiaires rejoignent Kantiles. En 2026, des locaux sont ouverts à Paris et Toulouse, pour se rapprocher des territoires et institutions partenaires.\nDès 2027, l’équipe accompagne les premiers dispositifs emploi-insertion et mène des missions pour la DREES, la DARES, plusieurs OPCO, des ministères, mais aussi des collectivités locales, en France et à l’international.\nLes projets se multiplient : analyses d’accès aux droits, mesure d’impact social, évaluation de politiques publiques, développement de tableaux de bord. En 2029, la création du KLab (laboratoire d’innovation sociale par les données) marque un tournant vers une approche plus ouverte, reproductible et collaborative.\n\n“Une donnée n’a de sens que si elle parle à ceux qu’elle concerne. Notre travail, c’est de la reconnecter aux réalités humaines et aux décisions concrètes.”\n— Justine Vroylandt, cofondatrice de Kantiles\n\n\n\n\nLes grandes étapes de Kantiles en un coup d’œil"
  },
  {
    "objectID": "index.html#café-consommé",
    "href": "index.html#café-consommé",
    "title": "Accueil",
    "section": "Café consommé",
    "text": "Café consommé\n\nEn moyenne 4 tasses / jour / personne\n\nEnviron 3 personnes caféinées par jour\n\nSur 5 mois (~22 jours/mois), ça fait environ 4 x 3 x 22 x 5 = 1 320 tasses"
  },
  {
    "objectID": "index.html#nombre-dentre-clients",
    "href": "index.html#nombre-dentre-clients",
    "title": "Accueil",
    "section": "Nombre d’entre-clients",
    "text": "Nombre d’entre-clients\n\nEstimé à 47 interactions différentes\nDu mail flou à la demande d’extraction sur mesure"
  },
  {
    "objectID": "index.html#vitesse-de-réaction",
    "href": "index.html#vitesse-de-réaction",
    "title": "Accueil",
    "section": "Vitesse de réaction",
    "text": "Vitesse de réaction\n\nIdée → pitch → prototypage : parfois dans la demi-journée\nLe record : une dataviz en 2h chrono 🏎️"
  },
  {
    "objectID": "index.html#projets-réalisés",
    "href": "index.html#projets-réalisés",
    "title": "Accueil",
    "section": "Projets réalisés",
    "text": "Projets réalisés\n\nAPEC\n\nDREES\n\nKids\n\nOPCO\n\nEndomind\n\n…"
  },
  {
    "objectID": "index.html#outils-méthodes",
    "href": "index.html#outils-méthodes",
    "title": "Accueil",
    "section": "Outils / Méthodes",
    "text": "Outils / Méthodes\n\nR, Quarto, Leaflet\n\nGit + GitHub, DuckDB\n\nScraping, analyses territoriales\n…"
  },
  {
    "objectID": "index.html#conférences-événements",
    "href": "index.html#conférences-événements",
    "title": "Accueil",
    "section": "Conférences / Événements",
    "text": "Conférences / Événements\n\nDéj POS – Impact & mesure\n\nRencontres R - Université de Mons\n\nPrintemps de l’économie\n\nINSEE Pays de la Loire\n\nColloque PIC - DARES\n\nJournée data & histoire - Sorbonne\n\nUne journée à 21\n\n…"
  },
  {
    "objectID": "api.html#introduction",
    "href": "api.html#introduction",
    "title": "API",
    "section": "",
    "text": "Lorsqu’on veut récuopérer des données sur internet, la première question à se poser est :\nExiste-t-il un service pour ?\nDans le cas le plus simple une base est disponible en accès libre (data.gouv, insee.fr, etc.). Si les informations ne sont pas disponibles, peut être existe-t-il un service de requête des données : une API (Application Programming Interface).\n\n\nUne API est une interface « clairement délimitée par laquelle un logiciel offre des services à d’autres logiciels ».\n\nChaque API est accompagnée d’une documentation. Il est souvent nécessaire de s’y référer pour former des requêtes valables.\nUne API attend des paramètres précis qui peuvent varier selon les données que l’on demande.\nNotre exemple : France Travail propose un certain nombre d’API pour des services différents et des clients différents (particuliers, employeurs, etc.). Nous nous intéresserons à l’API qui donne accès à des informations territoriales recensées par France Travail\n\nSi R n’est pas le meilleur langage pour requêter des API, il existe quelques packages qui facilitent le travail. Nous utiliserons httr2."
  },
  {
    "objectID": "api.html#se-connecter-à-lapi-france-travail",
    "href": "api.html#se-connecter-à-lapi-france-travail",
    "title": "API",
    "section": "Se connecter à l’API France Travail",
    "text": "Se connecter à l’API France Travail\nLa procédure y est décrite, il faut donc se connecter et cliquer sur utiliser l’api. On peut alors créer une clé d’identification (= un token).\nPour éviter d’écrire notre clé d’identification dans notre script qui sera potentiellement partagé ou déposé sur Github, nous pouvons (devons !) la mettre dans le fichier local d’environnement. On appelle ce fichier avec usethis::edit_r_environ()et on entre notre clé ainsi que l’identifiant :\n\nclient_ftio_key=“PAR_applitest_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx” mdp_ftio_key=“fafxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx”\n\nOn enregistre le fichier et on relance le projet pour que la modification soit prise en compte. On pourra alors accéder à ces clés avec la commande Sys.getenv(\"client_ftio_key\")"
  },
  {
    "objectID": "api.html#construction-de-la-requête-avec-httr2",
    "href": "api.html#construction-de-la-requête-avec-httr2",
    "title": "API",
    "section": "Construction de la requête avec httr2",
    "text": "Construction de la requête avec httr2\nIl nous faut maintenant construire notre requête. La première page de documentation nous donne deux informations importantes pour identifier l’API que l’on va requêter : Scopes : api_stats-informations-territoirev1,infosterritoire Royaume : /partenaire Gardons les dans un coin pour l’instant. Sur la gauche, plusieurs types d’informations sont disponibles, choisissons Population/ Stats sur la population totale (POP_1). La page nous donne alors la marche à suivre avec toutes les informations nécessaires pour former notre requête et les spécifications facultatives. Il nous faut commencer par indiquer l’url à laquelle on va faire notre demande.\nlibrary(httr2)\n\nlien &lt;- \"https://api.francetravail.io/partenaire/stats-informations-territoire/v1/indicateur/stat-population\"\nrequest(lien)\nUne requête a deux parties : un header et un body. Le header contient les informations de connexion et les métadonnées (type de contenu par exemple).\nrequest(lien) |&gt;\n  req_headers(\n    Accept = \"application/json\",\n    Authorization = \"Bearer -xXXxxXXxxXXxxXXx\"\n  )\nIci on prend la clé temporaire qui nous est fournie (elle n’est valide que quelques minutes). Ensuite on ajoute les spécifications nécessaires dans la partie « body ».\nrequest(lien) |&gt;\n  req_headers(\n    Accept = \"application/json\",\n    Authorization = \"Bearer -xXXxxXXxxXXxxXXx\"\n  ) |&gt;\n  req_body_json(\n    list(\n      \"codeTypeTerritoire\" = \"REG\",\n      \"codeTerritoire\" = \"75\",\n      \"codeTypeActivite\" = \"CUMUL\",\n        \"codeActivite\" = \"CUMUL\",\n        \"codeTypePeriode\" = \"ANNEE\"\n       )\n  )\nA ce niveau, notre requête est terminée ! Mais elle n’a pas encore été envoyée, nous n’avons fait que la construire. Il nous suffit de rentrer la requête dans la fonction req_perform() qui donne ce que renvoie l’API, donc on met tout ça dans un objet pour le conserver.\ndf_pop_na &lt;- request(\n  lien\n) |&gt;\n  req_headers(\n    Accept = \"application/json\",\n    Authorization = \"Bearer -xXXxxXXxxXXxxXXx\"\n  ) |&gt;\n  req_body_json(\n    list(\n      \"codeTypeTerritoire\" = \"REG\",\n      \"codeTerritoire\" = \"75\",\n      \"codeTypeActivite\" = \"CUMUL\",\n      \"codeActivite\" = \"CUMUL\",\n      \"codeTypePeriode\" = \"ANNEE\"\n    )\n  ) |&gt;\n  req_perform()\nNous voilà avec un json que l’on peut transformer en data.frame. Cette étape dépend du format renvoyé par chaque api.\nTable_pop_na &lt;- map(\n  resp_body_json(page),\n  as_tibble\n) |&gt;\n  bind_rows()"
  },
  {
    "objectID": "api.html#automatisation-de-la-requête",
    "href": "api.html#automatisation-de-la-requête",
    "title": "API",
    "section": "Automatisation de la requête",
    "text": "Automatisation de la requête\nL’intérêt d’une API est de pouvoir requêter des données automatiquement. Utiliser la clé temporaire n’est donc pas le mieux. Nous allons devoir demander régulièrement une clé pour pouvoir requêter.\nPour cela, il faut nous intéresser à la cinématique de France travail. Il s’agit d’une discussion entre plusieurs parties dont l’utilisateur et le service requêté, afin d’avoir l’autorisation de requêter. (Un schéma ici)\nIl va donc falloir faire une première requête qui a pour unique objectif de recevoir une clé pour faire notre requête de données. Cette première requête va nous permettre de nous identifier auprès de l’API. La page que nous avons ouvert nous donne précisément ce qu’il faut indiquer.\nL’url de requête doit contenir le royaume (realm) que l’on a récupéré au début.\nlien_key &lt;- \"https://francetravail.io/connexion/oauth2/access_token?realm=%2Fpartenaire\"\nrequete &lt;- request(lien_key)\nEnsuite on intègre toutes les informations demandées :\n\nNos identifiants que l’on a mis dans l’environnement local\nLe grant_type\nLe scope que l’on a également récupéré au début\n\nrequete &lt;- request(\n  lien_key\n) |&gt;\n  req_body_form(\n    client_id = Sys.getenv(\"client_ftio_key\"),\n    client_secret = Sys.getenv(\"mdp_ftio_key\"),\n    grant_type = \"client_credentials\",\n    scope = \"api_stats-informations-territoirev1 infosterritoire\"\n  ) |&gt;\n  req_perform()\nOn obtient un objet qui contient un token (la fameuse clé). Pour la voir il suffit d’entrer resp_body_json(requete)$access_token.\npage &lt;- request(\n  lien\n) |&gt;\n  req_auth_bearer_token(token = resp_body_json(requete)$access_token) |&gt;\n# equivalent à “Authorization = \"Bearer xXXxxXXxxXXxxXXx\"” que l’on avait mis dans le header, mais on y a mis la clé que l’on vient de demander.\n  req_headers(\n    Accept = \"application/json\"\n  ) |&gt;\n  req_body_json(\n    list(\n    \"codeTypeTerritoire\" = \"REG\",\n    \"codeTerritoire\" = \"75\",\n    \"codeTypeActivite\" = \"CUMUL\",\n    \"codeActivite\" = \"CUMUL\",\n    \"codeTypePeriode\" = \"ANNEE\"\n    )\n  )  |&gt;\n  req_perform(verbosity = 2)\n# verbosity = 2 permet d’avoir plus d’information sur ce qu’il se passe\nOn a donc une demande de clé automatisée (plus besoin de rafraîchir notre navigateur et de copier/coller le bearer xxxx). Maintenant, la clé reste temporaire, il faut relancer la première requête de demande de clé dès qu’elle n’est plus valide. Httr2 permet de le faire aussi automatiquement.\npage &lt;- request(lien) |&gt;\n \n# Ce module permet de gérer automatiquement la demande de clé :\n# Remarquez que la fonction ne fonctionne que pour le grant_type « credential », il existe d’autres fonction pour d’autres types.\n\n  req_oauth_client_credentials(\n    client = oauth_client(\n    id = Sys.getenv(\"client_ftio_key\"),\n    secret = Sys.getenv(\"mdp_ftio_key\"),\n    token_url = lien_key\n    ),\n    scope = \"api_stats-informations-territoirev1 infosterritoire\"\n  ) |&gt;\n# On y intègre toutes les informations nécessaires et il se débrouille\n \n# Pour ne faire que 10 requêtes par seconde (maximum autorisé par France Travail), on peut utiliser le throttling :\n  req_throttle(\n    capacity = 10,\n    fill_time_s = 1\n  ) |&gt;\n \n  req_headers(\n    Accept = \"application/json\"\n  ) |&gt;\n  req_body_json(\n    list(\n    \"codeTypeTerritoire\" = \"REG\",\n    \"codeTerritoire\" = \"11\",\n    \"codeTypeActivite\" = \"MOYENNE\",\n    \"codeActivite\" = \"MOYENNE\",\n    \"codeTypePeriode\" = \"TRIMESTRE\"\n    )\n  ) |&gt;\n  req_perform(verbosity = 2)"
  },
  {
    "objectID": "quarto.html#vue-générale",
    "href": "quarto.html#vue-générale",
    "title": "Quarto et Markdown",
    "section": "",
    "text": "Quarto repose sur Pandoc : convertisseur universel Markdown vers plusieurs formats (HTML, PDF, Word, etc.).\nÉcrit en Rust, moderne et rapide.\nSupporte plusieurs langages dans les chunks : R, Python, Observable JS, etc."
  },
  {
    "objectID": "quarto.html#workflow",
    "href": "quarto.html#workflow",
    "title": "Quarto et Markdown",
    "section": "Workflow",
    "text": "Workflow\n\nQuarto exécute les chunks de code, produit un AST (Abstract Syntax Tree).\nPandoc convertit cet AST en documents finaux (HTML, PDF, etc.)."
  },
  {
    "objectID": "quarto.html#comparaison-avec-r-markdown",
    "href": "quarto.html#comparaison-avec-r-markdown",
    "title": "Quarto et Markdown",
    "section": "Comparaison avec R Markdown",
    "text": "Comparaison avec R Markdown\n\n\n\n\n\n\n\n\nFonctionnalité\nR Markdown\nQuarto\n\n\n\n\nLangage\nR + Markdown\nRust + Markdown\n\n\nSupport chunks\nR uniquement\nR, Python, JS, autres\n\n\nParamétrage YAML\nBasique\nAvancé\n\n\nSorties\nHTML, PDF, Word\nHTML, PDF, Word, et plus"
  },
  {
    "objectID": "quarto.html#création-pdf",
    "href": "quarto.html#création-pdf",
    "title": "Quarto et Markdown",
    "section": "Création PDF",
    "text": "Création PDF\n\nPlusieurs méthodes :\n\nLaTeX classique (fiable, mais lourd parfois)\nTypst (nouveau langage, plus rapide)\nPageJS (génération PDF via Chrome)\nImpression design via HTML/Figma"
  },
  {
    "objectID": "quarto.html#exemple-de-chunk-r",
    "href": "quarto.html#exemple-de-chunk-r",
    "title": "Quarto et Markdown",
    "section": "Exemple de chunk R",
    "text": "Exemple de chunk R\n\n#library(readxl)\n#tables &lt;- read_excel(\"tables_exported.xlsx\")\n#head(tables)"
  },
  {
    "objectID": "quarto.html#latex",
    "href": "quarto.html#latex",
    "title": "Quarto et Markdown",
    "section": "1. LaTeX",
    "text": "1. LaTeX\n\nMoteur historique, très utilisé en économie, physique, et sciences formelles.\nComplexe à utiliser, peu flexible graphiquement.\nIntégration dans Quarto via engine: pdf avec pdf-engine: lualatex.\nPermet une très grande personnalisation, mais au prix d’une courbe d’apprentissage raide."
  },
  {
    "objectID": "quarto.html#typst",
    "href": "quarto.html#typst",
    "title": "Quarto et Markdown",
    "section": "2. Typst",
    "text": "2. Typst\n\nÉcrit en Rust, pensé comme une alternative rapide et moderne à LaTeX.\nSyntaxe simple, compilation ultra-rapide (quelques ms).\nIntégré directement dans Quarto via engine: typst.\nEncore jeune (2 ans), mais très prometteur.\nParfait pour des cas d’usage type rapport régulier, document structuré, avec un design simple.\n\nTypst est particulièrement adapté aux workflows automatisés en R ou Python, pour produire rapidement des documents PDF paramétrés."
  },
  {
    "objectID": "quarto.html#css-paged-media-paged.js-weasyprint-pdf-raptor",
    "href": "quarto.html#css-paged-media-paged.js-weasyprint-pdf-raptor",
    "title": "Quarto et Markdown",
    "section": "3. CSS Paged Media (paged.js, weasyprint, pdf-raptor)",
    "text": "3. CSS Paged Media (paged.js, weasyprint, pdf-raptor)\n\nGénère d’abord un HTML avec CSS.\nUtilise des standards CSS Media Queries + des polyfills pour structurer les pages comme des documents imprimables.\nCompatible avec Chrome/Edge, et utilisable via impression PDF.\nExemples :\n\npaged.js\nweasyprint (en Python)\npdf-raptor (service web de conversion)"
  },
  {
    "objectID": "quarto.html#génération-manuelle-via-figma-ou-html-pur",
    "href": "quarto.html#génération-manuelle-via-figma-ou-html-pur",
    "title": "Quarto et Markdown",
    "section": "4. Génération manuelle via Figma ou HTML pur",
    "text": "4. Génération manuelle via Figma ou HTML pur\n\nUtilisé pour un design sur-mesure pixel-perfect.\nGénère le HTML ou maquette à la main, ou via outils comme Figma.\nBypasse entièrement Pandoc ou Quarto.\nNécessite plus de maintenance, mais offre un contrôle absolu."
  },
  {
    "objectID": "quarto.html#paramètres-définis-dans-len-tête-yaml",
    "href": "quarto.html#paramètres-définis-dans-len-tête-yaml",
    "title": "Quarto et Markdown",
    "section": "Paramètres définis dans l’en-tête YAML",
    "text": "Paramètres définis dans l’en-tête YAML\nparams:\n  country: \"Somalia\"\n  sector: \"Éducation\"\n  unit: \"Région\""
  },
  {
    "objectID": "quarto.html#réutilisation-dans-le-texte",
    "href": "quarto.html#réutilisation-dans-le-texte",
    "title": "Quarto et Markdown",
    "section": "Réutilisation dans le texte",
    "text": "Réutilisation dans le texte\nLes paramètres définis dans l’en-tête YAML peuvent être appelés dynamiquement avec params$… .\n\n# glue(\"Ce rapport analyse le secteur **{params$sector}** pour le pays **{params$country}**, à l’échelle **{params$unit}**.\")"
  },
  {
    "objectID": "quarto.html#bonnes-pratiques-pour-un-rapport-quarto-paramétré",
    "href": "quarto.html#bonnes-pratiques-pour-un-rapport-quarto-paramétré",
    "title": "Quarto et Markdown",
    "section": "Bonnes pratiques pour un rapport Quarto paramétré",
    "text": "Bonnes pratiques pour un rapport Quarto paramétré\n\nRecommandé\n\nUtiliser des blocs logiques bien identifiés (#, ##, ###)\nGarder une structure constante (même ordre, mêmes titres)\nUtiliser des paramètres dynamiques pour l’automatisation\nAjouter des graphiques avec ggplot2 ou plotly\nSoigner l’affichage des nombres (arrondis, formatés)\nGérer les valeurs manquantes ou aberrantes\n\n\n\nÀ éviter\n\nTrop de colonnes dans les mises en page (complexe avec Pandoc)\nDes titres qui changent selon les paramètres (illisible à la lecture)\nNe pas tester les cas extrêmes (ex. : 0 valeur, NA, territoire inexistant)\nTrop de verbosité automatique (“Le pays {x} a… le pays {y} a…”)"
  },
  {
    "objectID": "quarto.html#styliser-avec-css-pour-html",
    "href": "quarto.html#styliser-avec-css-pour-html",
    "title": "Quarto et Markdown",
    "section": "Styliser avec CSS (pour HTML)",
    "text": "Styliser avec CSS (pour HTML)\n\nFichier : styles.css\n:root {\n  --main-color: #0077b6;\n  --accent-color: #48cae4;\n  --font-body: 'Inter', sans-serif;\n}\n\nbody {\n  font-family: var(--font-body);\n  color: #222;\n  line-height: 1.6;\n}\n\nh1, h2, h3 {\n  color: var(--main-color);\n  margin-top: 1.5em;\n}\n\nstrong {\n  color: var(--accent-color);\n}"
  }
]