


---
title: "Rapport complet : Recensements, Quarto, Scraping, PDF et Zotero"
author: "Utilisateur"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-summary: "Afficher le code R"
    theme: cosmo
  pdf:
    toc: true
    number-sections: true
    latex-engine: xelatex
    keep-tex: true
---

# Introduction

Ce rapport rassemble les notes détaillées sur plusieurs thématiques clés dans le traitement des données, la gestion documentaire, et la production de rapports.  
Les sujets abordés sont :

- Recensements et gestion de grandes bases de données
- Utilisation avancée de Quarto et Markdown
- Techniques de scraping en R
- Lecture et traitement de PDF en R
- Organisation documentaire avec Zotero

---

# test

## Rapport annuel {.panel-tabset}

### Introduction

Ceci est l’introduction du rapport.

### Données {.panel-collapse}

```{r}
summary(mtcars)
```

# Recensements

## Description des fichiers

- Deux fichiers principaux : `ointerrogationsprincipales` et un fichier complémentaire.
- Non diffusés directement au public, servent à produire plusieurs fichiers diffusés.
- Données au niveau des cantons, villes, avec parfois localisation précise (unité Iris).
- Utilisation pour mobilités professionnelles et scolaires.

## Formats de diffusion

- Fichiers diffusés en CSV et Parquet (format optimisé, indexé, rapide).
- Parquet de plus en plus utilisé en statistique.
- Données lourdes : environ 26 millions de lignes par fichier.
- Une ligne coûte en ressources ~3 francs (évaluation de coût traitement).

## Outils pour manipuler

- Dugdibby : base de données sur serveur permettant lecture et tri.
- Installation locale + serveur distant.
- Base orientée colonnes, optimisée pour analyse.

## Import et manipulation en R

- Utilisation de fonctions `glue()` pour assembler chaînes de caractères, notamment pour importer.
- Utilisation de `here::here()` pour gérer chemins absolus et relatifs, éviter erreurs liées aux chemins.
- Exemple d'import rapide :

```r
library(here)
data_path <- here("data", "recensement.parquet")
```



# Quarto et Markdown

## Vue générale

- Quarto repose sur **Pandoc** : convertisseur universel Markdown vers plusieurs formats (HTML, PDF, Word, etc.).
- Écrit en Rust, moderne et rapide.
- Supporte plusieurs langages dans les chunks : R, Python, Observable JS, etc.

## Workflow

- Quarto exécute les chunks de code, produit un AST (Abstract Syntax Tree).
- Pandoc convertit cet AST en documents finaux (HTML, PDF, etc.).

## Comparaison avec R Markdown

| Fonctionnalité        | R Markdown                  | Quarto                        |
|----------------------|-----------------------------|------------------------------|
| Langage              | R + Markdown                | Rust + Markdown              |
| Support chunks       | R uniquement                | R, Python, JS, autres         |
| Paramétrage YAML     | Basique                    | Avancé                       |
| Sorties              | HTML, PDF, Word             | HTML, PDF, Word, et plus      |

## Création PDF

- Plusieurs méthodes :
  - LaTeX classique (fiable, mais lourd parfois)
  - Typst (nouveau langage, plus rapide)
  - PageJS (génération PDF via Chrome)
  - Impression design via HTML/Figma

## Exemple de chunk R

```{r}
#library(readxl)
#tables <- read_excel("tables_exported.xlsx")
#head(tables)

```




---



# Scraping Web avec R

## Introduction

Le **scraping web** consiste à extraire automatiquement des données publiées sur des sites internet. Cette méthode est particulièrement utile lorsque les données ne sont pas disponibles via une API officielle ou sous un format exploitable directement (CSV, JSON, XML, etc.). Elle permet de collecter un large éventail d’informations : fiches produits, listes, tableaux, statistiques, organigrammes, documents, etc.

Cependant, le scraping nécessite une bonne maîtrise des technologies web (HTML, CSS, JavaScript) ainsi qu’une attention aux aspects légaux et éthiques (respect du fichier robots.txt, limitations du site, charge serveur).

---

## Pourquoi faire du scraping ?

- **Collecter des données non accessibles autrement**, notamment sur des sites qui ne proposent pas d’API ou de fichiers exportables.  
- **Automatiser la veille documentaire** en récupérant régulièrement les nouvelles publications d’un site.  
- **Récupérer des informations structurées dans des pages HTML complexes**, par exemple des tableaux ou des fiches détaillées.  
- **Enrichir des jeux de données locaux** avec des données publiques sectorielles, comme des organigrammes ou listes d’acteurs.

---

## Outils et bibliothèques en R

- **`rvest`** : bibliothèque principale pour lire, parser et extraire du contenu HTML. Utilise les sélecteurs CSS ou XPath pour cibler précisément les éléments à extraire.  
- **`xml2`** : permet de lire et manipuler des documents XML (utile pour lire des sitemaps par exemple).  
- **`httr` ou `curl`** : pour gérer les requêtes HTTP, parfois nécessaires pour simuler des sessions, gérer des cookies, ou contourner des protections.  
- **`purrr`** : fonctionnel pour itérer sur des listes d’URLs et appliquer des fonctions de scraping.  
- **Extensions Chrome (Selector Gadget, DevTools)** : outils indispensables pour identifier rapidement les sélecteurs CSS et tester les expressions XPath.

---

## Bonnes pratiques avant de commencer

- **Vérifier le fichier `robots.txt`** du site (ex. https://www.site.com/robots.txt) qui liste les parties autorisées ou interdites au scraping. Respecter ces consignes est crucial pour éviter tout litige légal.  
- **Analyser la structure du site** : repérer les pages d’index, les pages détaillées, les liens à suivre, et identifier les sélecteurs CSS/XPath précis.  
- **Consulter le sitemap XML** si disponible, car il facilite la récupération automatique des URLs à scraper.  
- **Limiter la fréquence des requêtes** pour ne pas surcharger le serveur (exemple : pause entre requêtes avec `Sys.sleep()`).  
- **Tester et valider chaque étape manuellement** avant d’automatiser à grande échelle.  
- **Documenter le script** pour faciliter la maintenance et la mise à jour si la structure du site change.

---

## Exemple complet : récupération et scraping de fiches sur cheese.com

### Étape 1 : récupération des URLs via sitemap

```{r}
library(xml2)
library(magrittr)

url_sitemap <- "https://www.cheese.com/sitemap.xml"
sitemap <- read_xml(url_sitemap)

# Extraction des URLs du sitemap
urls <- xml_find_all(sitemap, "//url/loc") %>% xml_text()
head(urls, 5)


library(rvest)
library(dplyr)

url_annuaire <- "https://www.cheese.com/all-cheeses/"
page <- read_html(url_annuaire)

# Extraction des liens vers chaque fiche
liens_fromages <- page %>%
  html_nodes(".cheeseList a") %>%
  html_attr("href")

# Nettoyage des liens pour avoir URL absolue
liens_fromages <- paste0("https://www.cheese.com", liens_fromages)
head(liens_fromages, 5)

library(rvest)
library(dplyr)

url_annuaire <- "https://www.cheese.com/all-cheeses/"
page <- read_html(url_annuaire)

# Extraction des liens vers chaque fiche
liens_fromages <- page %>%
  html_nodes(".cheeseList a") %>%
  html_attr("href")

# Nettoyage des liens pour avoir URL absolue
liens_fromages <- paste0("https://www.cheese.com", liens_fromages)
head(liens_fromages, 5)

```

















# Lecture de PDF en R

## Outils

- **tesseract** : outil OCR pour extraire du texte à partir d’images contenues dans des PDF.
- **tabulizer** (package R) : extraction de tableaux depuis des PDF, particulièrement efficace pour les tableaux “screenshot” (captures d’écran intégrées dans PDF).
- **pdftools** : manipulation basique et extraction de texte dans les PDF.

## Usages

- Extraction de tableaux complexes, notamment lorsque les données sont intégrées en image (screenshot).
- Le package `tabulizer` est utile pour extraire ces tableaux, contrairement à l’OCR qui fonctionne moins bien sur ce type de données.
- Vérifier la cohérence des tailles de listes extraites (ex. colonnes de tableaux) pour éviter les erreurs de décalage ou pertes de données.
- Gérer les valeurs manquantes, car en HTML/texte brut, il n’y a pas de NA natif.
- Utile pour enrichir des données sectorielles ou locales à partir de rapports PDF.

## Cas pratique

- Tester sur des sites institutionnels comme l’INSEE en live.
- Nettoyer et structurer les données extraites pour créer des jeux de données propres.
- Être vigilant aux problèmes liés à la mise en forme des PDF (listes, colonnes, espacements).


```{r}
# library(pdftools)

# Exemple d'extraction texte d'un PDF local
# texte_pdf <- pdf_text("exemple_rapport.pdf")

# Affichage des 500 premiers caractères de la première page
# cat(substr(texte_pdf[1], 1, 500))
```


```{r}
# library(tabulizer)

# Extraction des tableaux d'un PDF
# tables <- extract_tables("exemple_rapport.pdf")

# Affichage du premier tableau extrait
# print(tables[[1]])

```

```{r}
# library(tesseract)

# Extraction de texte depuis une image (capture ou PDF image)
# texte_ocr <- ocr("exemple_image_page.png")

# cat(texte_ocr)
```

---














# Zotero : gestion documentaire

## Module : Zotero & Veille documentaire
- Juin 2025
- Emmanuel Herbepin et Victoire Chatain

---

## Présentation générale de Zotero
- Zotero est un outil de gestion bibliographique gratuit, open source, et très adapté pour :  
  - La veille documentaire structurée.  
  - La gestion collaborative de références (études, articles, rapports, etc.).  
  - La constitution d’une base de connaissance dynamique.

---

## Fonctionnement de base
- Organisation par dossiers et sous-dossiers.  
- Ajout de documents :  
  - Depuis un plugin navigateur (fonctionne très bien avec Cairn, par exemple).  
  - Manuellement (ajout de PDF, capture de page, etc.).  
- Chaque document peut contenir :  
  - Des métadonnées (titre, auteur, date…).  
  - Des pièces jointes (PDF, notes, etc.).  
  - Des commentaires et surlignages collaboratifs.

---

## Travail collaboratif
- Bibliothèque de groupe (via Zotero Web) :  
  - Paramétrage des droits par membre.  
  - Synchronisation automatique.  
  - Partage des annotations visibles par tous (identification de l’auteur, surlignages, commentaires).  
- Utilisable comme un Drive académique orienté contenu structuré.

---

## Fonctionnalités utiles
- Tags / marqueurs pour organiser les contenus.  
- Génération automatique de bibliographies (choix du format).  
- Format de citation personnalisable (utile pour livrables, publications, rapports).  
- Format BibTeX disponible → interopérable avec R, Python, LaTeX.

---

## Cas d’usage professionnel : veille sur l'emploi & la formation
- Objectif :  
  Construire une base documentaire cohérente, exhaustive, structurée, réutilisable et dynamique sur les thématiques métier.  
- Exemples :  
  - Études sur les compétences dans les métiers de la santé.  
  - Publications régionales et nationales : DREES, DARES, INSEE, France Travail, etc.  
  - Mémoires, études locales, travaux des observatoires OPCO, InterCarifOref, France Compétences…  
- Organisation :  
  - Hiérarchisation des sources.  
  - Sélection raisonnée des institutions clés.  
  - Veille structurée sur 15 000 à 20 000 études (sur 20 ans) de 4 à 8 pages chacune.

---

## Méthodologie recommandée
- Étape 1 : Constitution du corpus  
  - Liste prioritaire : DREES, DARES, INSEE, France Travail, Trésor, OPCO.  
  - Acteurs régionaux : InterCarifOref, collectivités territoriales.  
  - Indexation manuelle + flux RSS + scraping raisonné.  
- Étape 2 : Intégration dans Zotero  
  - Par scraping ou via export BibTeX → import dans Zotero.  
  - Utilisation des API Zotero et/ou packages dédiés pour automatiser.  
- Étape 3 : Structuration documentaire  
  - Tags thématiques.  
  - Catégorisation par année, région, institution.  
  - Export possible vers livrable interne ou outil de visualisation (type Quarto ou R Markdown).

---

## Vers une base documentaire dynamique
- Objectif : passer du statique au dynamique.  
- Intégrer les études dans une base réinterrogeable (dashboard, moteur de recherche, analyse de tendance…).  
- Base centrale pour fonction observatoire interne.  
- Exemples d’usages :  
  - Liste des études par année.  
  - Focus régional ou sectoriel.  
  - Suivi des politiques publiques ou signaux faibles.

---

## À creuser / Aller plus loin
- API Zotero pour automatiser des chargements et extractions.  
- Connecteurs avec Zotero depuis R (`rbbt`, `zoteroR`) ou Python.  
- Scraping bibliographique ciblé sur les bons sites.  
- Intégration avec un outil de visualisation dynamique (Quarto, Observable, Shiny…).



---

# Annexes : exemples de code R

## Lecture et extraction de sitemap XML

```{r}
library(rvest)
library(xml2)

url_sitemap <- "https://www.cheese.com/sitemap.xml"
sitemap <- read_xml(url_sitemap)
urls <- xml_find_all(sitemap, "//url/loc") %>% xml_text()
head(urls)
```



## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:
